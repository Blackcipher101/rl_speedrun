# Value Functions and Policies

This document defines policies and value functions, the core concepts for evaluating and comparing decision strategies in MDPs.

---

## Prerequisites

This document assumes familiarity with:
- MDP definition $\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$
- Return $G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$

---

## 1. Policies

A **policy** specifies how an agent selects actions. We distinguish between stochastic and deterministic policies.

### 1.1 Stochastic Policy

**Definition**: A stochastic policy $\pi$ is a mapping from states to probability distributions over actions:

$$\pi: \mathcal{S} \to \Delta(\mathcal{A})$$

where $\Delta(\mathcal{A})$ denotes the probability simplex over $\mathcal{A}$.

For each state $s \in \mathcal{S}$ and action $a \in \mathcal{A}$:

$$\pi(a \mid s) \triangleq \Pr\{A_t = a \mid S_t = s\}$$

**Properties**:
1. $\pi(a \mid s) \geq 0$ for all $s \in \mathcal{S}$, $a \in \mathcal{A}$
2. $\sum_{a \in \mathcal{A}} \pi(a \mid s) = 1$ for all $s \in \mathcal{S}$

### 1.2 Deterministic Policy

**Definition**: A deterministic policy $\pi$ is a mapping from states to actions:

$$\pi: \mathcal{S} \to \mathcal{A}$$

We write $\pi(s)$ to denote the action taken in state $s$.

**Relation to stochastic policies**: A deterministic policy is a special case where:

$$\pi(a \mid s) = \begin{cases} 1 & \text{if } a = \pi(s) \\ 0 & \text{otherwise} \end{cases}$$

### 1.3 Stationary vs. Non-Stationary Policies

**Stationary policy**: The mapping $\pi$ does not depend on time $t$. The action distribution $\pi(\cdot \mid s)$ is the same regardless of when state $s$ is visited.

**Non-stationary policy**: The policy may depend on time: $\pi_t(a \mid s)$.

**Important result**: For infinite-horizon discounted MDPs with $\gamma < 1$, there always exists an optimal policy that is both **deterministic** and **stationary**. This is why we focus on such policies.

---

## 2. State-Value Function

The state-value function quantifies the expected return starting from a given state and following a specific policy.

### 2.1 Definition

**Definition**: The state-value function $V^\pi: \mathcal{S} \to \mathbb{R}$ for policy $\pi$ is:

$$V^\pi(s) \triangleq \mathbb{E}_\pi[G_t \mid S_t = s]$$

Expanding the definition of $G_t$:

$$V^\pi(s) = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \,\bigg|\, S_t = s\right]$$

**Notation**: The subscript $\pi$ in $\mathbb{E}_\pi$ indicates that the expectation is taken over trajectories generated by following policy $\pi$:
- Actions are sampled according to $A_t \sim \pi(\cdot \mid S_t)$
- States transition according to $S_{t+1} \sim P(\cdot \mid S_t, A_t)$

### 2.2 Explicit Expansion

We can expand $V^\pi(s)$ by explicitly writing out the expectations over actions and next states.

**Step 1**: Condition on the first action $A_t$:

$$V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a \mid s) \cdot \mathbb{E}_\pi[G_t \mid S_t = s, A_t = a]$$

**Step 2**: Condition on the next state $S_{t+1}$:

$$V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a \mid s) \sum_{s' \in \mathcal{S}} P(s' \mid s, a) \cdot \mathbb{E}_\pi[G_t \mid S_t = s, A_t = a, S_{t+1} = s']$$

**Step 3**: Use $G_t = R_{t+1} + \gamma G_{t+1}$ and the Markov property:

$$V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a \mid s) \sum_{s' \in \mathcal{S}} P(s' \mid s, a) \left[R(s, a, s') + \gamma \cdot \mathbb{E}_\pi[G_{t+1} \mid S_{t+1} = s']\right]$$

**Step 4**: Recognize that $\mathbb{E}_\pi[G_{t+1} \mid S_{t+1} = s'] = V^\pi(s')$:

$$\boxed{V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a \mid s) \sum_{s' \in \mathcal{S}} P(s' \mid s, a) \left[R(s, a, s') + \gamma V^\pi(s')\right]}$$

This is the **Bellman expectation equation** for $V^\pi$, which we will study in detail in the next module.

---

## 3. Action-Value Function

The action-value function quantifies the expected return starting from a state, taking a specific action, and then following a policy.

### 3.1 Definition

**Definition**: The action-value function $Q^\pi: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$ for policy $\pi$ is:

$$Q^\pi(s, a) \triangleq \mathbb{E}_\pi[G_t \mid S_t = s, A_t = a]$$

**Interpretation**: $Q^\pi(s, a)$ is the expected return if we:
1. Start in state $s$
2. Take action $a$ (potentially different from what $\pi$ would choose)
3. Follow policy $\pi$ thereafter

### 3.2 Explicit Expansion

**Step 1**: Condition on the next state:

$$Q^\pi(s, a) = \sum_{s' \in \mathcal{S}} P(s' \mid s, a) \cdot \mathbb{E}_\pi[G_t \mid S_t = s, A_t = a, S_{t+1} = s']$$

**Step 2**: Use $G_t = R_{t+1} + \gamma G_{t+1}$:

$$Q^\pi(s, a) = \sum_{s' \in \mathcal{S}} P(s' \mid s, a) \left[R(s, a, s') + \gamma \cdot \mathbb{E}_\pi[G_{t+1} \mid S_{t+1} = s']\right]$$

**Step 3**: The future return from $s'$ depends on the policy:

$$\mathbb{E}_\pi[G_{t+1} \mid S_{t+1} = s'] = \sum_{a' \in \mathcal{A}} \pi(a' \mid s') \cdot Q^\pi(s', a')$$

Therefore:

$$\boxed{Q^\pi(s, a) = \sum_{s' \in \mathcal{S}} P(s' \mid s, a) \left[R(s, a, s') + \gamma \sum_{a' \in \mathcal{A}} \pi(a' \mid s') Q^\pi(s', a')\right]}$$

This is the **Bellman expectation equation** for $Q^\pi$.

---

## 4. Relationship Between $V^\pi$ and $Q^\pi$

The state-value and action-value functions are closely related.

### 4.1 From $Q^\pi$ to $V^\pi$

The state-value is the expected action-value under the policy:

$$V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a \mid s) \cdot Q^\pi(s, a) = \mathbb{E}_{a \sim \pi(\cdot|s)}[Q^\pi(s, a)]$$

**Derivation**:
$$V^\pi(s) = \mathbb{E}_\pi[G_t \mid S_t = s] = \sum_{a} \pi(a \mid s) \cdot \mathbb{E}_\pi[G_t \mid S_t = s, A_t = a] = \sum_{a} \pi(a \mid s) \cdot Q^\pi(s, a)$$

**For deterministic policies** $\pi$: $V^\pi(s) = Q^\pi(s, \pi(s))$

### 4.2 From $V^\pi$ to $Q^\pi$

The action-value is the expected immediate reward plus discounted future state-value:

$$Q^\pi(s, a) = \sum_{s' \in \mathcal{S}} P(s' \mid s, a) \left[R(s, a, s') + \gamma V^\pi(s')\right]$$

**Derivation**: This follows directly from the expansion in Section 3.2 with $V^\pi(s') = \sum_{a'} \pi(a' \mid s') Q^\pi(s', a')$.

### 4.3 Summary of Relationships

```
         π(a|s)
V^π(s) ←-------- Q^π(s,a)
   |                 |
   | Backup          | Backup
   ↓                 ↓
Q^π(s,a)          V^π(s')
```

$$V^\pi(s) = \sum_a \pi(a \mid s) \cdot Q^\pi(s, a)$$

$$Q^\pi(s, a) = \sum_{s'} P(s' \mid s, a) \left[R(s, a, s') + \gamma V^\pi(s')\right]$$

---

## 5. Optimal Value Functions

### 5.1 Optimal State-Value Function

**Definition**: The optimal state-value function $V^*: \mathcal{S} \to \mathbb{R}$ is:

$$V^*(s) \triangleq \max_\pi V^\pi(s) \quad \forall s \in \mathcal{S}$$

**Interpretation**: $V^*(s)$ is the maximum expected return achievable from state $s$ by any policy.

### 5.2 Optimal Action-Value Function

**Definition**: The optimal action-value function $Q^*: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$ is:

$$Q^*(s, a) \triangleq \max_\pi Q^\pi(s, a) \quad \forall s \in \mathcal{S}, a \in \mathcal{A}$$

**Interpretation**: $Q^*(s, a)$ is the maximum expected return achievable by taking action $a$ in state $s$ and then acting optimally.

### 5.3 Relationship Between $V^*$ and $Q^*$

$$V^*(s) = \max_{a \in \mathcal{A}} Q^*(s, a)$$

**Derivation**: Let $\pi^*$ be an optimal policy. Then:
$$V^*(s) = V^{\pi^*}(s) = \max_a Q^{\pi^*}(s, a) = \max_a Q^*(s, a)$$

The second equality uses the fact that the optimal policy is greedy with respect to $Q^{\pi^*}$.

And:

$$Q^*(s, a) = \sum_{s' \in \mathcal{S}} P(s' \mid s, a) \left[R(s, a, s') + \gamma V^*(s')\right]$$

---

## 6. Optimal Policies

### 6.1 Definition

**Definition**: A policy $\pi^*$ is **optimal** if:

$$V^{\pi^*}(s) \geq V^\pi(s) \quad \forall s \in \mathcal{S}, \forall \pi$$

Equivalently: $V^{\pi^*} = V^*$.

### 6.2 Existence Theorem

**Theorem**: For any finite MDP, there exists at least one optimal policy. Moreover, there exists an optimal policy that is deterministic and stationary.

**Proof sketch**:
1. The set of stationary deterministic policies is finite (at most $m^n$ policies where $m = |\mathcal{A}|$, $n = |\mathcal{S}|$)
2. Each policy has a well-defined value function
3. The maximum over a finite set exists
4. It can be shown that this maximum is also optimal over all (including stochastic) policies

### 6.3 Extracting Optimal Policy from $Q^*$

Given $Q^*$, an optimal policy is:

$$\pi^*(s) = \arg\max_{a \in \mathcal{A}} Q^*(s, a)$$

If there are ties, any maximizing action yields an optimal policy.

### 6.4 Extracting Optimal Policy from $V^*$

Given $V^*$ and the model $(P, R)$:

$$\pi^*(s) = \arg\max_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} P(s' \mid s, a) \left[R(s, a, s') + \gamma V^*(s')\right]$$

**Note**: Unlike $Q^*$, extracting the policy from $V^*$ requires knowledge of the transition dynamics $P$ and reward function $R$.

---

## 7. Policy Ordering

### 7.1 Partial Ordering on Policies

**Definition**: Policy $\pi$ is **better than or equal to** policy $\pi'$, written $\pi \geq \pi'$, if:

$$V^\pi(s) \geq V^{\pi'}(s) \quad \forall s \in \mathcal{S}$$

This defines a partial ordering because some policies may be incomparable (one better in some states, worse in others).

### 7.2 Existence of a Best Policy

**Theorem**: For any finite MDP, there exists at least one policy $\pi^*$ such that $\pi^* \geq \pi$ for all policies $\pi$.

This is non-trivial: a priori, one might expect that different policies could be best in different states, with no single policy dominating everywhere. The theorem states this doesn't happen for MDPs.

---

## 8. Value Function Bounds

### 8.1 Lower and Upper Bounds

For bounded rewards with $|R(s, a, s')| \leq R_{\max}$:

$$\frac{-R_{\max}}{1 - \gamma} \leq V^\pi(s) \leq \frac{R_{\max}}{1 - \gamma} \quad \forall s, \pi$$

**Proof** (upper bound):
$$V^\pi(s) = \mathbb{E}_\pi\left[\sum_{k=0}^\infty \gamma^k R_{t+k+1}\right] \leq \sum_{k=0}^\infty \gamma^k R_{\max} = \frac{R_{\max}}{1 - \gamma}$$

The lower bound follows similarly.

### 8.2 Span of Value Functions

The **span** of $V$:

$$\text{span}(V) \triangleq \max_s V(s) - \min_s V(s) \leq \frac{2 R_{\max}}{1 - \gamma}$$

---

## 9. Matrix-Vector Formulation

For computational purposes, we often express value functions in matrix-vector form.

### 9.1 Notation

- $\mathbf{V}^\pi \in \mathbb{R}^n$: Vector with $[\mathbf{V}^\pi]_i = V^\pi(s_i)$
- $\mathbf{P}^\pi \in \mathbb{R}^{n \times n}$: Transition matrix under policy $\pi$
  - $[\mathbf{P}^\pi]_{ij} = \sum_a \pi(a \mid s_i) P(s_j \mid s_i, a)$
- $\mathbf{r}^\pi \in \mathbb{R}^n$: Expected immediate reward vector
  - $[\mathbf{r}^\pi]_i = \sum_a \pi(a \mid s_i) \sum_{s'} P(s' \mid s_i, a) R(s_i, a, s')$

### 9.2 Matrix Form of Bellman Equation

The Bellman expectation equation in matrix form:

$$\mathbf{V}^\pi = \mathbf{r}^\pi + \gamma \mathbf{P}^\pi \mathbf{V}^\pi$$

**Closed-form solution** (for policy evaluation):

$$\mathbf{V}^\pi = (\mathbf{I} - \gamma \mathbf{P}^\pi)^{-1} \mathbf{r}^\pi$$

The matrix $(\mathbf{I} - \gamma \mathbf{P}^\pi)$ is invertible when $\gamma < 1$ because its eigenvalues have magnitude less than 1.

---

## 10. Summary Table

| Concept | Definition | Key Property |
|---------|------------|--------------|
| Stochastic policy | $\pi(a \mid s)$ | $\sum_a \pi(a \mid s) = 1$ |
| Deterministic policy | $\pi: \mathcal{S} \to \mathcal{A}$ | $a = \pi(s)$ |
| State-value | $V^\pi(s) = \mathbb{E}_\pi[G_t \mid S_t = s]$ | Expected return from state |
| Action-value | $Q^\pi(s,a) = \mathbb{E}_\pi[G_t \mid S_t = s, A_t = a]$ | Expected return from state-action |
| Optimal state-value | $V^*(s) = \max_\pi V^\pi(s)$ | Best achievable return |
| Optimal action-value | $Q^*(s,a) = \max_\pi Q^\pi(s,a)$ | Best return after taking $a$ |
| Optimal policy | $\pi^*(s) = \arg\max_a Q^*(s,a)$ | Greedy w.r.t. $Q^*$ |

---

## References

1. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press. Chapter 3.
2. Puterman, M. L. (2014). *Markov Decision Processes*. Wiley. Chapters 5-6.
